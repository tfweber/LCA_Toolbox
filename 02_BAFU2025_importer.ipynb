{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6381256",
   "metadata": {},
   "source": [
    "In this file, we want to import BAFU:2025 to our LCA_Toolbox brightway project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9336eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic packages from brightway\n",
    "import bw2analyzer as ba\n",
    "import bw2calc as bc\n",
    "import bw2data as bd\n",
    "import bw2io as bi\n",
    "from bw2io.importers import SingleOutputEcospold2Importer\n",
    "import bw2analyzer as bwa\n",
    "from bw2data import methods\n",
    "\n",
    "# other relevant packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36ac5452",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_file = Path(r\"C:\\Users\\TimWeber\\databases\\BAFU 2025 ecospold\\elementary_flows_mapping.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad269c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "bafu_files = Path(r\"C:\\Users\\TimWeber\\databases\\BAFU 2025 ecospold\\BAFU-2025 ecospold1\\BAFU-2025_LCI ecoSpold v1 (for other softwares)\\LCI ecoSpold v1 Files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8fe5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "bd.projects.set_current('LCA_Toolbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cb19ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databases dictionary with 7 object(s):\n",
       "\tbiosphere3\n",
       "\tbw25_db\n",
       "\tecoinvent-3.10-biosphere\n",
       "\tecoinvent-3.11-biosphere\n",
       "\tecoinvent-3.11-consequential\n",
       "\tecoinvent-3.12-biosphere\n",
       "\tecoinvent-3.12-consequential"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd.databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "33439da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference index contains 11747 entries.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from lxml import etree\n",
    "import re\n",
    "\n",
    "ECOSPOLD_DIR = Path(bafu_files)  # root folder with .xml\n",
    "DB_NAME = \"bafu\"\n",
    "\n",
    "NAME_LOC_RE = re.compile(r\"^(.*)\\s+\\{([^{}]+)\\}\\s*$\")\n",
    "\n",
    "def strip_location_from_name(name, current_location=None):\n",
    "    \"\"\"\n",
    "    If name ends with ' {...}', strip this part and return (clean_name, location).\n",
    "    Location from name overrides current_location if found.\n",
    "    \"\"\"\n",
    "    if not name:\n",
    "        return name, current_location\n",
    "    m = NAME_LOC_RE.match(name)\n",
    "    if not m:\n",
    "        return name, current_location\n",
    "    base, loc_str = m.groups()\n",
    "    base = base.strip()\n",
    "    loc = loc_str.strip()\n",
    "    return base, (loc or current_location)\n",
    "\n",
    "def get_first(elem, xpath):\n",
    "    \"\"\"Helper: return first match or None.\"\"\"\n",
    "    res = elem.xpath(xpath)\n",
    "    return res[0] if res else None\n",
    "\n",
    "def get_attr(elem, name, default=None):\n",
    "    return elem.get(name) if elem is not None and elem.get(name) is not None else default\n",
    "\n",
    "\n",
    "def build_reference_index(root_dir: Path):\n",
    "    \"\"\"\n",
    "    First pass: for each dataset, find its main production exchange\n",
    "    and build an index keyed by that exchange's 'number' (flow id).\n",
    "\n",
    "    Returns: dict[flow_id] = {\n",
    "        'activity_code': dataset_number,\n",
    "        'name': clean_name,\n",
    "        'reference product': clean_name,\n",
    "        'location': location,\n",
    "        'unit': unit,\n",
    "    }\n",
    "    \"\"\"\n",
    "    ref_index = {}\n",
    "\n",
    "    for xml in root_dir.rglob(\"*.xml\"):\n",
    "        tree = etree.parse(str(xml))\n",
    "        root = tree.getroot()\n",
    "\n",
    "        dataset = get_first(root, '//*[local-name()=\"dataset\"]')\n",
    "        if dataset is None:\n",
    "            continue\n",
    "\n",
    "        PI = get_first(dataset, './/*[local-name()=\"processInformation\"]')\n",
    "        RF = get_first(PI, './/*[local-name()=\"referenceFunction\"]') if PI is not None else None\n",
    "        geo = get_first(PI, './/*[local-name()=\"geography\"]') if PI is not None else None\n",
    "\n",
    "        if RF is None:\n",
    "            continue\n",
    "\n",
    "        raw_name = get_attr(RF, \"name\", \"\") or \"\"\n",
    "        unit = get_attr(RF, \"unit\", \"\")\n",
    "        xml_location = get_attr(geo, \"location\", None) or \"GLO\"\n",
    "        dataset_number = get_attr(dataset, \"number\", \"\")\n",
    "\n",
    "        # Clean name and location\n",
    "        clean_name, location = strip_location_from_name(raw_name, xml_location)\n",
    "\n",
    "        # Find main production exchange (outputGroup == \"0\")\n",
    "        flow_data = get_first(dataset, './/*[local-name()=\"flowData\"]')\n",
    "        prod_flow_id = None\n",
    "\n",
    "        if flow_data is not None:\n",
    "            for exc in flow_data.xpath('./*[local-name()=\"exchange\"]'):\n",
    "                og = get_first(exc, './*[local-name()=\"outputGroup\"]')\n",
    "                og_val = og.text.strip() if og is not None and og.text else None\n",
    "                if og_val == \"0\":\n",
    "                    prod_flow_id = get_attr(exc, \"number\", None)\n",
    "                    if prod_flow_id:\n",
    "                        break\n",
    "\n",
    "        # Fallback: if we didn't find a production exchange number, use dataset number\n",
    "        flow_id = prod_flow_id or dataset_number\n",
    "        if not flow_id:\n",
    "            continue\n",
    "\n",
    "        ref_index[flow_id] = {\n",
    "            \"activity_code\": dataset_number,\n",
    "            \"name\": clean_name,\n",
    "            \"reference product\": clean_name,\n",
    "            \"location\": location,\n",
    "            \"unit\": unit,\n",
    "        }\n",
    "\n",
    "    return ref_index\n",
    "\n",
    "# Build the index once\n",
    "ref_index = build_reference_index(ECOSPOLD_DIR)\n",
    "print(f\"Reference index contains {len(ref_index)} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc99fef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 11747 dataset dictionaries.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from stats_arrays.distributions import (\n",
    "    LognormalUncertainty,\n",
    "    NormalUncertainty,\n",
    "    TriangularUncertainty,\n",
    "    UniformUncertainty,\n",
    "    UndefinedUncertainty,\n",
    ")\n",
    "\n",
    "\n",
    "def person_label(person_elem):\n",
    "    \"\"\"Return a compact 'Name (email)' label from a <person> element.\"\"\"\n",
    "    if person_elem is None:\n",
    "        return None\n",
    "    name = (get_attr(person_elem, \"name\", \"\") or \"\").strip()\n",
    "    email = (get_attr(person_elem, \"email\", \"\") or \"\").strip()\n",
    "    if name and email:\n",
    "        return f\"{name} ({email})\"\n",
    "    elif name:\n",
    "        return name\n",
    "    elif email:\n",
    "        return email\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_comment(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # Decode HTML entities like &#10;\n",
    "    t = html.unescape(text)\n",
    "\n",
    "    # Drop trailing \"UUID: ...\" noise, if present\n",
    "    t = re.sub(r\"UUID:.*$\", \"\", t, flags=re.S)\n",
    "\n",
    "    # Normalise line breaks and remove empty lines\n",
    "    lines = [ln.strip() for ln in t.replace(\"\\r\\n\", \"\\n\").split(\"\\n\")]\n",
    "    lines = [ln for ln in lines if ln]  # drop completely empty lines\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def add_uncertainty_fields_from_ecospold1(exc_xml, data: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Mutates & returns `data` by adding Brightway-style uncertainty fields\n",
    "    from an ecospold1 <exchange> element.\n",
    "    \"\"\"\n",
    "\n",
    "    # ecoSpold1 uncertaintyType (0..4)\n",
    "    try:\n",
    "        uncertainty = int(exc_xml.get(\"uncertaintyType\", 0))\n",
    "    except ValueError:\n",
    "        uncertainty = 0\n",
    "\n",
    "    def floatish(x):\n",
    "        if x is None:\n",
    "            return np.NaN\n",
    "        try:\n",
    "            return float(x.strip())\n",
    "        except Exception:\n",
    "            try:\n",
    "                return float(x)\n",
    "            except Exception:\n",
    "                return np.NaN\n",
    "\n",
    "    mean = floatish(exc_xml.get(\"meanValue\"))\n",
    "    min_ = floatish(exc_xml.get(\"minValue\"))\n",
    "    max_ = floatish(exc_xml.get(\"maxValue\"))\n",
    "    sigma95 = floatish(exc_xml.get(\"standardDeviation95\"))\n",
    "\n",
    "    # Some data has nonsense lognormal parameters -> treat as undefined\n",
    "    if uncertainty == 1 and (sigma95 in (0, 1) or np.isnan(sigma95)):\n",
    "        uncertainty = 0\n",
    "\n",
    "    # ---------- LOGNORMAL ----------\n",
    "    if uncertainty == 1:\n",
    "        # Guard against mean <= 0 explicitly; lognormal not defined there\n",
    "        if mean == 0 or np.isnan(mean):\n",
    "            data.update(\n",
    "                {\n",
    "                    \"uncertainty type\": UndefinedUncertainty.id,\n",
    "                    \"amount\": float(mean),\n",
    "                    \"loc\": float(mean),\n",
    "                }\n",
    "            )\n",
    "            return data\n",
    "\n",
    "        # Normal lognormal case\n",
    "        data.update(\n",
    "            {\n",
    "                \"uncertainty type\": LognormalUncertainty.id,  # ID 2\n",
    "                \"amount\": float(mean),\n",
    "                \"loc\": math.log(abs(mean)),\n",
    "                \"scale\": math.log(math.sqrt(float(sigma95))),\n",
    "                \"negative\": mean < 0,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Extra sanity check: bad scale -> fall back to undefined\n",
    "        if np.isnan(data[\"scale\"]):\n",
    "            data[\"uncertainty type\"] = UndefinedUncertainty.id\n",
    "            data[\"loc\"] = data[\"amount\"]\n",
    "            data.pop(\"scale\", None)\n",
    "\n",
    "        return data\n",
    "\n",
    "    # ---------- NORMAL ----------\n",
    "    if uncertainty == 2:\n",
    "        data.update(\n",
    "            {\n",
    "                \"uncertainty type\": NormalUncertainty.id,  # ID 3\n",
    "                \"amount\": float(mean),\n",
    "                \"loc\": float(mean),\n",
    "                \"scale\": float(sigma95) / 2.0,\n",
    "            }\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    # ---------- TRIANGULAR ----------\n",
    "    if uncertainty == 3:\n",
    "        data.update(\n",
    "            {\n",
    "                \"uncertainty type\": TriangularUncertainty.id,  # ID 5\n",
    "                \"minimum\": float(min_),\n",
    "                \"maximum\": float(max_),\n",
    "            }\n",
    "        )\n",
    "        most_likely = floatish(exc_xml.get(\"mostLikelyValue\"))\n",
    "        if not np.isnan(most_likely):\n",
    "            data[\"amount\"] = data[\"loc\"] = most_likely\n",
    "        else:\n",
    "            data[\"amount\"] = data[\"loc\"] = float(mean)\n",
    "        return data\n",
    "\n",
    "    # ---------- UNIFORM ----------\n",
    "    if uncertainty == 4:\n",
    "        data.update(\n",
    "            {\n",
    "                \"uncertainty type\": UniformUncertainty.id,  # ID 4\n",
    "                \"amount\": float(mean),\n",
    "                \"minimum\": float(min_),\n",
    "                \"maximum\": float(max_),\n",
    "            }\n",
    "        )\n",
    "        return data\n",
    "\n",
    "    # ---------- UNDEFINED / NO UNCERTAINTY ----------\n",
    "    data.update(\n",
    "        {\n",
    "            \"uncertainty type\": UndefinedUncertainty.id,  # ID 0\n",
    "            \"amount\": float(mean),\n",
    "            \"loc\": float(mean),\n",
    "        }\n",
    "    )\n",
    "    return data\n",
    "\n",
    "UNITS_MAP = {\n",
    "    'kg': 'kilogram',\n",
    "    'tkm': 'ton kilometer',\n",
    "    'p': 'unit',\n",
    "    'kWh': 'kilowatt hour',\n",
    "    'MJ': 'megajoule',\n",
    "    'm2': 'square meter',\n",
    "    'm': 'meter',\n",
    "    'Nm3': 'cubic meter',\n",
    "    'km': 'kilometer',\n",
    "    'personkm': 'person-kilometer',\n",
    "    'my': 'meter-year',\n",
    "    'unit': 'unit',\n",
    "    'm3': 'cubic meter',\n",
    "    'm2a': 'square meter-year',\n",
    "    'kmy': 'kilometer-year',\n",
    "    'a': 'year',\n",
    "    'm3y': 'cubic meter-year',\n",
    "    'kBq': 'kilo Becquerel',\n",
    "    'ha': 'hectare',\n",
    "    'Bq': 'Becquerel',\n",
    "    'hr': 'hour'\n",
    "}\n",
    "\n",
    "CATS_MAP = {\n",
    "    ('emissions to air', 'unspecified'): ('air',),\n",
    "    ('emissions to air', 'high. pop.'): ('air', 'urban air close to ground'),\n",
    "    ('emissions to air', 'low. pop.'): ('air', 'non-urban air or from high stacks'),\n",
    "    ('emissions to air', 'stratosphere + troposphere'): ('air', 'lower stratosphere + upper troposphere'),\n",
    "    ('emissions to air', 'low. pop., long-term'): ('air', 'low population density, long-term'),\n",
    "    ('emissions to air', 'indoor'): ('air', 'urban air close to ground'),\n",
    "    \n",
    "    ('emissions to soil', 'unspecified'): ('soil',),\n",
    "    ('emissions to soil', 'forestry'): ('soil', 'forestry'),\n",
    "    ('emissions to soil', 'agricultural'): ('soil', 'agricultural'),\n",
    "    ('emissions to soil', 'industrial'): ('soil', 'industrial'),\n",
    "    \n",
    "    ('emissions to water', 'ocean'): ('water', 'ocean'),\n",
    "    ('emissions to water', 'river'): ('water', 'surface water'),\n",
    "    ('emissions to water', 'unspecified'): ('water',),\n",
    "    ('emissions to water', 'groundwater, long-term'): ('water', 'ground-, long-term'),\n",
    "    ('emissions to water', 'groundwater'): ('water', 'ground-'),\n",
    "    ('emissions to water', 'lake'): ('water', 'surface water'),\n",
    "    ('emissions to water', 'river, long-term'): ('water', 'surface water'),\n",
    "    ('emissions to water', 'fossilwater'): ('water', 'fossil well'),\n",
    "    \n",
    "    ('economic issues', 'unspecified'): ('economic', 'primary production factor'),\n",
    "\n",
    "    ('resources', 'in ground'): ('natural resource', 'in ground'),\n",
    "    ('resources', 'land'): ('natural resource', 'land'),\n",
    "    ('resources', 'in water'): ('natural resource', 'in water'),\n",
    "    ('resources', 'in air'): ('natural resource', 'in air'),\n",
    "    ('resources', 'biotic'): ('natural resource', 'biotic'),\n",
    "}\n",
    "\n",
    "def parse_ecospold_file(xml_path: Path, db_name: str, ref_index: dict):\n",
    "    \"\"\"Parse a single ecoSpold1 file into a Brightway-like dict, using ref_index.\"\"\"\n",
    "    tree = etree.parse(str(xml_path))\n",
    "    root = tree.getroot()\n",
    "\n",
    "    dataset = get_first(root, '//*[local-name()=\"dataset\"]')\n",
    "    if dataset is None:\n",
    "        return None\n",
    "\n",
    "    PI = get_first(dataset, './/*[local-name()=\"processInformation\"]')\n",
    "    RF = get_first(PI, './/*[local-name()=\"referenceFunction\"]') if PI is not None else None\n",
    "    geo = get_first(PI, './/*[local-name()=\"geography\"]') if PI is not None else None\n",
    "    TP = get_first(PI, './/*[local-name()=\"timePeriod\"]') if PI is not None else None\n",
    "\n",
    "    if RF is None:\n",
    "        return None\n",
    "\n",
    "    # Basic fields\n",
    "    raw_name = get_attr(RF, \"name\", \"\") or \"\"\n",
    "    unit = UNITS_MAP.get(get_attr(RF, \"unit\", \"\"))\n",
    "    xml_location = get_attr(geo, \"location\", None) or \"GLO\"\n",
    "    dataset_number = get_attr(dataset, \"number\", \"\")\n",
    "\n",
    "    # Clean activity name and location (strip trailing {LOC})\n",
    "    name, location = strip_location_from_name(raw_name, xml_location)\n",
    "    ref_product = name\n",
    "\n",
    "    # Build comment from various ecoSpold1 fields, then clean it\n",
    "        # --- Build enriched comment from RF + timePeriod + metaInformation ---\n",
    "\n",
    "    rf_comment = get_attr(RF, \"generalComment\", \"\") or \"\"\n",
    "\n",
    "    # Time period: text + explicit start/end dates if present\n",
    "    tp_text = get_attr(TP, \"text\", \"\") or \"\"\n",
    "    tp_start_elem = get_first(TP, './/*[local-name()=\"startDate\"]') if TP is not None else None\n",
    "    tp_end_elem = get_first(TP, './/*[local-name()=\"endDate\"]') if TP is not None else None\n",
    "    tp_start = (tp_start_elem.text or \"\").strip() if tp_start_elem is not None and tp_start_elem.text else \"\"\n",
    "    tp_end = (tp_end_elem.text or \"\").strip() if tp_end_elem is not None and tp_end_elem.text else \"\"\n",
    "\n",
    "    time_lines = []\n",
    "    if tp_text:\n",
    "        time_lines.append(f\"Time period: {tp_text}\")\n",
    "    if tp_start or tp_end:\n",
    "        if tp_start and tp_end:\n",
    "            time_lines.append(f\"Time period (data): {tp_start} – {tp_end}\")\n",
    "        elif tp_start:\n",
    "            time_lines.append(f\"Time period (data): from {tp_start}\")\n",
    "        elif tp_end:\n",
    "            time_lines.append(f\"Time period (data): until {tp_end}\")\n",
    "\n",
    "    # Geography line: location + textual description if any\n",
    "    geo_text = get_attr(geo, \"text\", \"\") or \"\"\n",
    "    geo_lines = []\n",
    "    if xml_location or geo_text:\n",
    "        if geo_text:\n",
    "            geo_lines.append(f\"Geography: {xml_location} – {geo_text}\")\n",
    "        else:\n",
    "            geo_lines.append(f\"Geography: {xml_location}\")\n",
    "\n",
    "    # Technology description\n",
    "    tech = get_first(PI, './/*[local-name()=\"technology\"]') if PI is not None else None\n",
    "    tech_text = get_attr(tech, \"text\", \"\") or \"\"\n",
    "    tech_lines = []\n",
    "    if tech_text:\n",
    "        tech_lines.append(f\"Technology: {tech_text}\")\n",
    "\n",
    "    # metaInformation (administrative + modelling & validation)\n",
    "    MI = get_first(dataset, './/*[local-name()=\"metaInformation\"]')\n",
    "    AI = get_first(MI, './/*[local-name()=\"administrativeInformation\"]') if MI is not None else None\n",
    "    MV = get_first(MI, './/*[local-name()=\"modellingAndValidation\"]') if MI is not None else None\n",
    "\n",
    "    meta_lines = []\n",
    "\n",
    "    # --- Representativeness & production volume ---\n",
    "    rep = get_first(MV, './/*[local-name()=\"representativeness\"]') if MV is not None else None\n",
    "    if rep is not None:\n",
    "        rep_parts = []\n",
    "\n",
    "        # some schemas use e.g. \"productionVolume\", others \"productionVolumeText\"\n",
    "        prod_vol = get_attr(rep, \"productionVolume\", \"\") or \"\"\n",
    "        sampling = get_attr(rep, \"samplingProcedure\", \"\") or \"\"\n",
    "        extrap = get_attr(rep, \"extrapolations\", \"\") or \"\"\n",
    "        unc_adj = get_attr(rep, \"uncertaintyAdjustments\", \"\") or \"\"\n",
    "\n",
    "        if prod_vol and prod_vol.lower() != \"na\":\n",
    "            rep_parts.append(f\"Production volume: {prod_vol}\")\n",
    "        if sampling and sampling.lower() != \"<null>\":\n",
    "            rep_parts.append(f\"Sampling: {sampling}\")\n",
    "        if extrap and extrap.lower() != \"<null>\":\n",
    "            rep_parts.append(f\"Extrapolations: {extrap}\")\n",
    "        if unc_adj and unc_adj.lower() != \"none\":\n",
    "            rep_parts.append(f\"Uncertainty adjustments: {unc_adj}\")\n",
    "\n",
    "        if rep_parts:\n",
    "            meta_lines.append(\"Representativeness: \" + \"; \".join(rep_parts))\n",
    "\n",
    "    # --- People: build index of persons in this dataset ---\n",
    "    persons = {}\n",
    "    if AI is not None:\n",
    "        for p in AI.xpath('.//*[local-name()=\"person\"]'):\n",
    "            num = get_attr(p, \"number\", None)\n",
    "            if num:\n",
    "                persons[num] = p\n",
    "\n",
    "        # Data entry person\n",
    "        de = get_first(AI, './/*[local-name()=\"dataEntryBy\"]')\n",
    "        de_num = get_attr(de, \"person\", None) if de is not None else None\n",
    "        de_label = person_label(persons.get(de_num))\n",
    "        if de_label:\n",
    "            meta_lines.append(f\"Data entry: {de_label}\")\n",
    "\n",
    "        # Data generator\n",
    "        dgp = get_first(AI, './/*[local-name()=\"dataGeneratorAndPublication\"]')\n",
    "        dgp_num = get_attr(dgp, \"person\", None) if dgp is not None else None\n",
    "        dgp_label = person_label(persons.get(dgp_num))\n",
    "        if dgp_label:\n",
    "            meta_lines.append(f\"Data generator: {dgp_label}\")\n",
    "\n",
    "    # --- Validation info ---\n",
    "    if MV is not None:\n",
    "        val = get_first(MV, './/*[local-name()=\"validation\"]')\n",
    "        if val is not None:\n",
    "            details = get_attr(val, \"proofReadingDetails\", \"\") or \"\"\n",
    "            validator_num = get_attr(val, \"proofReadingValidator\", None)\n",
    "            validator_label = person_label(persons.get(validator_num)) if validator_num else None\n",
    "\n",
    "            if validator_label:\n",
    "                meta_lines.append(f\"Proof-reading validator: {validator_label}\")\n",
    "            if details:\n",
    "                meta_lines.append(f\"Validation details: {details}\")\n",
    "\n",
    "    # --- Source / publication info ---\n",
    "    if MV is not None:\n",
    "        src = get_first(MV, './/*[local-name()=\"source\"]')\n",
    "        if src is not None:\n",
    "            first_author = get_attr(src, \"firstAuthor\", \"\") or \"\"\n",
    "            add_authors = get_attr(src, \"additionalAuthors\", \"\") or \"\"\n",
    "            year = get_attr(src, \"year\", \"\") or \"\"\n",
    "            title = get_attr(src, \"title\", \"\") or \"\"\n",
    "            publisher = get_attr(src, \"publisher\", \"\") or \"\"\n",
    "            place = get_attr(src, \"placeOfPublications\", \"\") or \"\"\n",
    "            volume = get_attr(src, \"volumeNo\", \"\") or \"\"\n",
    "\n",
    "            src_parts = []\n",
    "            if first_author:\n",
    "                src_parts.append(first_author)\n",
    "            if add_authors:\n",
    "                # compress \"Frischknecht R., Stolz P.\" instead of listing everything verbosely\n",
    "                src_parts.append(add_authors)\n",
    "            if year:\n",
    "                src_parts.append(f\"({year})\")\n",
    "            if title:\n",
    "                src_parts.append(title)\n",
    "            if publisher or place or volume:\n",
    "                pub_bits = \", \".join(\n",
    "                    [x for x in [publisher, place, f\"vol. {volume}\" if volume else \"\"] if x]\n",
    "                )\n",
    "                if pub_bits:\n",
    "                    src_parts.append(pub_bits)\n",
    "\n",
    "            if src_parts:\n",
    "                meta_lines.append(\"Source: \" + \" \".join(src_parts))\n",
    "\n",
    "    # --- Combine everything and clean it ---\n",
    "    comment_sections = []\n",
    "\n",
    "    if rf_comment:\n",
    "        comment_sections.append(rf_comment)\n",
    "\n",
    "    comment_sections.extend(time_lines)\n",
    "    comment_sections.extend(geo_lines)\n",
    "    comment_sections.extend(tech_lines)\n",
    "    comment_sections.extend(meta_lines)\n",
    "\n",
    "    comment = clean_comment(\"\\n\".join(cs for cs in comment_sections if cs))\n",
    "\n",
    "\n",
    "    # Simple classifications\n",
    "    category = get_attr(RF, \"category\", None)\n",
    "    subcategory = get_attr(RF, \"subCategory\", None)\n",
    "    classifications = []\n",
    "    if category or subcategory:\n",
    "        classifications.append(\n",
    "            (\"EcoSpold01Categories\", f\"{category or ''}/{subcategory or ''}\")\n",
    "        )\n",
    "\n",
    "    # Exchanges\n",
    "    flow_data = get_first(dataset, './/*[local-name()=\"flowData\"]')\n",
    "    exchanges = []\n",
    "\n",
    "    if flow_data is not None:\n",
    "        for exc in flow_data.xpath('./*[local-name()=\"exchange\"]'):\n",
    "            # We still require a numeric meanValue; skip if missing\n",
    "            mean_value = get_attr(exc, \"meanValue\", None)\n",
    "            if mean_value is None:\n",
    "                continue\n",
    "\n",
    "            raw_ex_name = get_attr(exc, \"name\", \"\") or \"\"\n",
    "            ex_unit = UNITS_MAP.get(get_attr(exc, \"unit\", \"\"))\n",
    "            ex_cat = get_attr(exc, \"category\", \"\")\n",
    "            ex_subcat = get_attr(exc, \"subCategory\", \"\")\n",
    "            ex_loc_xml = get_attr(exc, \"location\", None)\n",
    "\n",
    "            # Strip location from exchange name\n",
    "            ex_name, ex_loc_from_name = strip_location_from_name(raw_ex_name, ex_loc_xml)\n",
    "            ex_loc = ex_loc_from_name\n",
    "\n",
    "            og = get_first(exc, './*[local-name()=\"outputGroup\"]')\n",
    "            ig = get_first(exc, './*[local-name()=\"inputGroup\"]')\n",
    "            og_val = og.text.strip() if og is not None and og.text else None\n",
    "            ig_val = ig.text.strip() if ig is not None and ig.text else None\n",
    "\n",
    "            ex_cat_lower = (ex_cat or \"\").lower()\n",
    "            ex_subcat_lower = (ex_subcat or \"\").lower()\n",
    "            \n",
    "            # Heuristics for biosphere categories\n",
    "            is_biosphere_category = (\n",
    "                ex_cat_lower.startswith(\"emissions to \")\n",
    "                or ex_cat_lower.startswith(\"emission to \")\n",
    "                or ex_cat_lower in {\"emissions\", \"emission\"}\n",
    "                or \"resource\" in ex_cat_lower\n",
    "                or (ex_cat, ex_subcat) in CATS_MAP\n",
    "            )\n",
    "            \n",
    "            # Lithium case: category 'resources', subcategory 'in ground'\n",
    "            # will satisfy \"resource\" in ex_cat_lower and be biosphere.\n",
    "            \n",
    "            if og_val == \"0\":\n",
    "                ex_type = \"production\"\n",
    "            elif is_biosphere_category:\n",
    "                ex_type = \"biosphere\"\n",
    "            elif ig_val is not None:\n",
    "                ex_type = \"technosphere\"\n",
    "            else:\n",
    "                # Fallback: if we have no input group and no explicit biosphere category,\n",
    "                # treat it as biosphere (this will catch odd emissions that lack inputGroup)\n",
    "                ex_type = \"biosphere\"\n",
    "\n",
    "\n",
    "            exc_dict = {\n",
    "                \"name\": ex_name,\n",
    "                \"unit\": ex_unit,\n",
    "                \"type\": ex_type,\n",
    "                \"categories\": CATS_MAP.get((ex_cat, ex_subcat), (ex_cat, ex_subcat)),\n",
    "            }\n",
    "\n",
    "            if ex_loc:\n",
    "                exc_dict[\"location\"] = ex_loc\n",
    "\n",
    "            # Optional per-exchange comment (also cleaned a bit)\n",
    "            ex_comment = exc.get(\"generalComment\")\n",
    "            if ex_comment:\n",
    "                exc_dict[\"comment\"] = clean_comment(ex_comment)\n",
    "\n",
    "            ex_number = get_attr(exc, \"number\", None)\n",
    "            if ex_number:\n",
    "                exc_dict[\"flow\"] = ex_number\n",
    "\n",
    "                # Inject reference product from the producer, if known\n",
    "                if ex_type in (\"technosphere\", \"production\"):\n",
    "                    ref_info = ref_index.get(ex_number)\n",
    "                    if ref_info is not None:\n",
    "                        exc_dict[\"reference product\"] = ref_info[\"reference product\"]\n",
    "                        if \"location\" not in exc_dict or not exc_dict[\"location\"]:\n",
    "                            exc_dict[\"location\"] = ref_info[\"location\"]\n",
    "\n",
    "            # Add Brightway-style uncertainty parameters\n",
    "            add_uncertainty_fields_from_ecospold1(exc, exc_dict)\n",
    "\n",
    "            exchanges.append(exc_dict)\n",
    "\n",
    "    data = {\n",
    "        \"database\": db_name,\n",
    "        \"code\": dataset_number,\n",
    "        \"name\": name,\n",
    "        \"reference product\": ref_product,\n",
    "        \"location\": location,\n",
    "        \"unit\": unit,\n",
    "        \"comment\": comment,\n",
    "        \"classifications\": classifications,\n",
    "        \"exchanges\": exchanges,\n",
    "        \"filename\": xml_path.name,\n",
    "        \"type\": \"process\",\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Second pass: build all dataset dicts\n",
    "all_data = []\n",
    "\n",
    "for xml in ECOSPOLD_DIR.rglob(\"*.xml\"):\n",
    "    dct = parse_ecospold_file(xml, DB_NAME, ref_index)\n",
    "    if dct is not None:\n",
    "        all_data.append(dct)\n",
    "\n",
    "print(f\"Built {len(all_data)} dataset dictionaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a31a6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1004 entries loaded.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = Path(mapping_file)\n",
    "\n",
    "\n",
    "def parse_category(cat_str):\n",
    "    cat_str = (cat_str or \"\").strip()\n",
    "    if not cat_str:\n",
    "        return None\n",
    "    return tuple(ast.literal_eval(cat_str))\n",
    "\n",
    "mapping = {}\n",
    "\n",
    "with open(CSV_PATH, newline=\"\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\",\")\n",
    "\n",
    "    # Fix BOM in header if present\n",
    "    if reader.fieldnames:\n",
    "        reader.fieldnames = [fn.lstrip(\"\\ufeff\") for fn in reader.fieldnames]\n",
    "\n",
    "    for row in reader:\n",
    "        bafu_name = row[\"BAFU name\"].strip()\n",
    "        bafu_cat = parse_category(row[\"BAFU category\"])\n",
    "\n",
    "        key = (bafu_name, bafu_cat)\n",
    "\n",
    "        mapping[key] = {\n",
    "            \"ecoinvent_name\": (row.get(\"Ecoinvent name\") or \"\").strip() or None,\n",
    "            \"ecoinvent_category\": parse_category(row.get(\"Ecoinvent category\")),\n",
    "        }\n",
    "\n",
    "print(f\"{len(mapping)} entries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b867fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in all_data:\n",
    "    for e in ds[\"exchanges\"]:\n",
    "        if e[\"type\"] == \"biosphere\":\n",
    "            key = (e[\"name\"], e[\"categories\"])\n",
    "            if key in mapping:\n",
    "                new_name = mapping[key]['ecoinvent_name']\n",
    "                new_cat = mapping[key]['ecoinvent_category']\n",
    "                if new_name:\n",
    "                    e[\"name\"] = new_name\n",
    "                if new_cat:\n",
    "                    e[\"categories\"] = new_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04a39e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = bw2io.importers.base_lci.LCIImporter(db_name=\"bafu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7b0d6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "i.data = all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "194dbe37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying strategy: normalize_units\n",
      "Applying strategy: drop_unspecified_subcategories\n",
      "Applying strategy: assign_only_product_as_production\n",
      "Applying strategy: strip_biosphere_exc_locations\n",
      "Applied 4 strategies in 0.68 seconds\n"
     ]
    }
   ],
   "source": [
    "i.apply_strategies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7db8b386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying strategy: link_iterable_by_fields\n",
      "Applying strategy: link_iterable_by_fields\n"
     ]
    }
   ],
   "source": [
    "i.match_database(fields=[\"name\", \"reference product\", \"location\"])\n",
    "i.match_database(\"biosphere3\", fields=[\"name\", \"categories\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03f87f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph statistics for `bafu` importer:\n",
      "11747 graph nodes:\n",
      "\tprocess: 11747\n",
      "417719 graph edges:\n",
      "\tbiosphere: 293189\n",
      "\ttechnosphere: 112783\n",
      "\tproduction: 11747\n",
      "408985 edges to the following databases:\n",
      "\tbiosphere3: 284455\n",
      "\tbafu: 124530\n",
      "246 unique unlinked edges (8734 total):\n",
      "\tbiosphere: 246\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(11747, 417719, 8734, 0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df0b8444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying strategy: drop_unlinked\n",
      "Applied 1 strategies in 0.11 seconds\n"
     ]
    }
   ],
   "source": [
    "i.drop_unlinked(i_am_reckless=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad00619e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:26:39+0100 [warning  ] Not able to determine geocollections for all datasets. This database is not ready for regionalization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11747/11747 [01:56<00:00, 100.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:28:43+0100 [info     ] Vacuuming database            \n",
      "Created database: bafu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Brightway2 SQLiteBackend: bafu"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i.write_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a77aa466",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Databases dictionary with 8 object(s):\n",
       "\tbafu\n",
       "\tbiosphere3\n",
       "\tbw25_db\n",
       "\tecoinvent-3.10-biosphere\n",
       "\tecoinvent-3.11-biosphere\n",
       "\tecoinvent-3.11-consequential\n",
       "\tecoinvent-3.12-biosphere\n",
       "\tecoinvent-3.12-consequential"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bd.databases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LCA_Toolbox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
